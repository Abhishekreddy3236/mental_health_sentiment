# ğŸ§  Mental Health Sentiment Monitor
### Social Media Big Data Analytics for Public Sentiment Monitoring

![Python](https://img.shields.io/badge/Python-3.9-blue) ![Spark](https://img.shields.io/badge/Apache%20Spark-4.0.2-orange) ![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-red) ![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-yellow)

A complete end-to-end NLP and Big Data pipeline that analyzes sentiment and emotions in Reddit mental health communities using three different AI models, topic modeling, real-time streaming, and distributed big data processing via Apache Spark.

---

## ğŸ“Œ Project Overview

This project builds a multi-model sentiment analysis system applied to Reddit mental health communities. It goes beyond binary sentiment by including emotion classification, topic modeling, and a live real-time data stream â€” all presented in an interactive Streamlit dashboard.

### Key Features
- **3-Model Sentiment Analysis** â€” VADER, Logistic Regression, and DistilBERT compared side by side
- **Emotion Classification** â€” 6 emotions (sadness, joy, fear, anger, love, surprise) across 5 communities
- **Topic Modeling** â€” BERTopic discovers latent themes without supervision
- **Big Data Processing** â€” Apache Spark 4.0 processes 1.6 million tweets
- **Real-Time Streaming** â€” Live HackerNews sentiment monitoring via public API
- **Interactive Dashboard** â€” 11-section Streamlit dashboard with filters and insights

---

## ğŸ—‚ï¸ Project Structure

```
mental_health_sentiment/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ reddit_mental_health.csv      # Raw Kaggle dataset (5,957 posts)
â”‚   â”œâ”€â”€ processed_data.csv            # After preprocessing (4,620 posts)
â”‚   â”œâ”€â”€ sentiment_results.csv         # After 3-model sentiment analysis
â”‚   â”œâ”€â”€ emotion_results.csv           # After emotion classification
â”‚   â”œâ”€â”€ final_results.csv             # Complete dataset with all features
â”‚   â””â”€â”€ live_stream.csv               # HackerNews real-time stream data
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lr_metrics.json               # Logistic Regression performance metrics
â”‚   â”œâ”€â”€ topic_info.csv                # BERTopic topic information
â”‚   â”œâ”€â”€ topic_words.json              # Top words per topic
â”‚   â”œâ”€â”€ spark_metrics.json            # PySpark MLlib metrics
â”‚   â”œâ”€â”€ bigdata_metrics.json          # Sentiment140 big data metrics
â”‚   â””â”€â”€ stream_summary.json           # Live stream summary statistics
â”‚
â”œâ”€â”€ 2_preprocess.py                   # Text cleaning and feature engineering
â”œâ”€â”€ 3_sentiment_models.py             # VADER + Logistic Regression + DistilBERT
â”œâ”€â”€ 4_emotion_classifier.py           # 6-emotion classification
â”œâ”€â”€ 5_topic_modeling.py               # BERTopic topic discovery
â”œâ”€â”€ 6_dashboard.py                    # Streamlit interactive dashboard
â”œâ”€â”€ 7_spark_analysis.py               # PySpark SQL + MLlib on Reddit data
â”œâ”€â”€ 8_spark_bigdata.py                # PySpark on Sentiment140 (1.6M tweets)
â”œâ”€â”€ 9_realtime_stream.py              # HackerNews live sentiment stream
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ§° Tech Stack

| Component | Technology |
|---|---|
| Language | Python 3.9 |
| Dashboard | Streamlit + Plotly |
| NLP Models | VADER, Scikit-learn, HuggingFace DistilBERT |
| Topic Modeling | BERTopic + SentenceTransformers |
| Big Data | Apache Spark 4.0 + Spark MLlib |
| Real-Time | HackerNews Firebase API |
| Data Source | Kaggle Reddit Mental Health Dataset |
| Big Data Source | Sentiment140 (1.6M tweets) |

---

## ğŸ“Š Data Sources

### 1. Reddit Mental Health Dataset (Primary)
- **Source:** Kaggle â€” Reddit Mental Health Dataset
- **Size:** 4,620 posts after preprocessing
- **Communities:** r/depression, r/anxiety, r/mentalhealth, r/SocialAnxiety, r/Mindfulness
- **Used for:** Full NLP pipeline â€” sentiment, emotion, topic modeling

### 2. Sentiment140 (Big Data Validation)
- **Source:** Kaggle â€” Sentiment140
- **Size:** 1,600,000 tweets
- **Used for:** Demonstrating Apache Spark at genuine big data scale
- **Note:** Not included in repo due to file size (230MB) â€” download separately

### 3. HackerNews Live API
- **Source:** HackerNews Firebase API (hacker-news.firebaseio.com)
- **Size:** 200 live posts per run
- **Used for:** Real-time sentiment monitoring demonstration
- **Note:** Free, no authentication required

---

## âš™ï¸ Setup & Installation

### Prerequisites
- Python 3.9
- Java 17 (required for PySpark)
- macOS / Linux

### Install Java 17 (macOS)
```bash
brew install openjdk@17
export JAVA_HOME=/opt/homebrew/opt/openjdk@17
export PATH=$JAVA_HOME/bin:$PATH
```

### Install Python Dependencies
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸš€ How to Run

### Run the Full Pipeline (first time only)
```bash
source venv/bin/activate
export JAVA_HOME=/opt/homebrew/opt/openjdk@17
export PATH=$JAVA_HOME/bin:$PATH

python3 2_preprocess.py          # Step 1: Clean data
python3 3_sentiment_models.py    # Step 2: Sentiment analysis
python3 4_emotion_classifier.py  # Step 3: Emotion classification
python3 5_topic_modeling.py      # Step 4: Topic modeling
python3 7_spark_analysis.py      # Step 5: PySpark on Reddit data
python3 8_spark_bigdata.py       # Step 6: PySpark on 1.6M tweets
python3 9_realtime_stream.py     # Step 7: Collect live HackerNews posts
```

### Launch the Dashboard
```bash
python3 -m streamlit run 6_dashboard.py
```
Open `http://localhost:8501` in your browser.

---

## ğŸ”¬ Models Used

### VADER
- Rule-based lexicon approach, no training required
- Fast but lacks contextual understanding
- **Result:** 59% negative, 37% positive, 4% neutral

### Logistic Regression (TF-IDF + Scikit-learn)
- TF-IDF features (10,000 features, 1-2 grams)
- Semi-supervised â€” trained on VADER labels
- **Accuracy:** 75.93%

### DistilBERT (Transformer)
- Pre-trained transformer fine-tuned on SST-2
- Understands full sentence context and nuance
- **Result:** 87% negative â€” correctly captures emotional complexity

### Why Three Models?
The disagreement analysis is the core academic contribution. Example: *"I'm desperate for a friend and to feel loved by someone"* â€” VADER and LR classify as **positive** (detected "friend" and "loved"), DistilBERT classifies as **negative** (understood desperation). This demonstrates why context-aware models are essential for mental health text.

---

## ğŸ“ˆ Key Findings

| Finding | Detail |
|---|---|
| Depression negativity | 92.6% negative â€” highest of all communities |
| Mindfulness surprise | Dominant emotion is **Fear (35.7%)**, not Joy |
| DistilBERT vs VADER | 87% vs 59% negative â€” context matters |
| Spark MLlib accuracy | 86.68% â€” outperforms scikit-learn LR (75.93%) |
| Big data scale | 1,280,209 training samples processed in 29.7 seconds |
| Topic: Loneliness | 1,273 posts, 89% negative â€” largest meaningful cluster |

---

## âš¡ Big Data Component

**Reddit Analytics (7_spark_analysis.py)**
- Spark SQL aggregations on community-level sentiment
- MLlib pipeline: 86.68% accuracy

**Sentiment140 Scale Validation (8_spark_bigdata.py)**
- 1,600,000 tweets loaded in **7.4 seconds**
- 1,280,209 training samples processed in **29.7 seconds**
- 76.72% accuracy on 319,791 test records

---

## ğŸ“¡ Real-Time Streaming

`9_realtime_stream.py` connects to the HackerNews public API and streams live posts, scoring each with VADER in real time.

**Live collection results:** 200 posts Â· 56.5% Neutral Â· 23.5% Positive Â· 20.0% Negative

---

## ğŸ“ Note on Large Files

`data/sentiment140.csv` (~230MB) is excluded from this repo. Download from Kaggle, place in `data/` folder, and run `8_spark_bigdata.py`.

All other data files are generated by running the pipeline scripts in order.

---

## ğŸ‘¨â€ğŸ’» Author

**Abhishek Reddy Kotha**
Built with Python 3.9 Â· Apache Spark 4.0 Â· HuggingFace Transformers Â· Streamlit
